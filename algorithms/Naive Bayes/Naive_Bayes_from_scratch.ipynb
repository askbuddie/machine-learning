{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ncZ89R_4xzR"
      },
      "source": [
        "# Naive Bayes Classifier From Scratch "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZnf4K1hYR1X"
      },
      "source": [
        "## Implementation\n",
        "\n",
        "Here is a brief description of the steps performed to implement the Naive Bayes model to classify the sentiment of product reviews:\n",
        "\n",
        "1. **Converting the text file**: The dataset text file is converted to a pandas dataframe. Each sentence in the dataset goes through some preprocessing steps.\n",
        "2. **7-fold Cross Validation**: Next, the dataframe is divided into 7 folds. For each of the 7 iterations, one of the set is considered as the testing set and the rest of the data is used in training.\n",
        "3. **Count Vectorization**: An array of unique words is derived from the training data. Each sentence in the training set is now fitted onto this array of unique words, giving is a matrix that will be used for training.\n",
        "4. **Fitting and evaluating the model**: Once we have a matrix representation for our training and testing data, we can fit the data into our model and use the Bayes theorem formula to derive the probabilities that a certain product review in our testing set is of positive sentiment. The predictions are compared to the actual labels for the testing data and accuracy is reported.\n",
        "5. **Deriving the final accuracy**: Once we have the accuracies for each of the 7 folds (testing sets), we can take their mean and report it. This will be the accuracy of our model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GfZZJ0A4cIeT"
      },
      "source": [
        "### Package Imports\n",
        "The following python packages have been used:\n",
        "1. **Numpy**: To represent data in a matrix form and to make vectorize mathematical operations to make them easier and faster.\n",
        "2. **Pandas**: To represent the dataset in an easy format (dataframe) and to simplify operations like splitting, merging, etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Csq6vObpxlA8"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AjrOI2Yhcrsk"
      },
      "source": [
        "### File Imports\n",
        "The dataset and stop words are read from their corresponding text files in the current directory. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YvTBFeQvzJJ6",
        "outputId": "56de09eb-d846-4c8c-9eb1-a08904d2fca1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Example: So there is no way for me to plug it in here in the US unless I go by a converter.  0\n",
            "Number of reviews: 1000\n"
          ]
        }
      ],
      "source": [
        "raw_data = np.array(open('dataset_NB.txt', 'r').read().split('\\n'))#returns a file object// mode read//default .read()-1 which reads the whole file//\\n is the seperator\n",
        "stop_words = set(np.array(open('stop_words.txt', 'r').read().split('\\n')))\n",
        "\n",
        "# Example review present in the dataset\n",
        "print('Example:', raw_data[0])\n",
        "# Number of reviews in the dataset\n",
        "count = len(raw_data)\n",
        "print('Number of reviews:', count)\n",
        "# Number of folds the data must be divided into\n",
        "FOLD_COUNT = 7"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSIXnArSd84J"
      },
      "source": [
        "### Data preprocessing functions\n",
        "The following functions are used to preprocess the data being given in the dataset. An explanation of what each function does is given alongside the code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKlmh10ZzesC"
      },
      "source": [
        "#### Note\n",
        "Since we noticed that the training data had a lot of spelling mistakes, we included some commented code that would use the \"autocorrect\" Python library to fix spelling mistakes (commented because libraries other than Numpy and Pandas are not allowed). Doing so increases the accuracy of the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "740u7e7BxLSN"
      },
      "outputs": [],
      "source": [
        "# !pip install autocorrect\n",
        "# from autocorrect import Speller\n",
        "# spell = Speller(lang = 'en')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UkIt1RWZ2OQe"
      },
      "outputs": [],
      "source": [
        "def remove_label(sentence):\n",
        "  \"\"\"\n",
        "  Input: A sentence picked out from the dataset with the label at the end.\n",
        "  Output: A tuple (sentence, label)\n",
        "  \"\"\"\n",
        "  return (sentence.strip()[: -1].strip(), sentence.strip()[-1])#slicing doesn't include the last term\n",
        "\n",
        "def preprocess_text(sentence):\n",
        "  \"\"\"\n",
        "  Input: A sentence obtained after removing the label.\n",
        "  Output: An array consisting of words of the sentence after converting to lowercase and removing punctuations and stop words.\n",
        "  \"\"\"\n",
        "  # convert the sentence to lowercase\n",
        "  sentence = sentence.lower()\n",
        "  # remove punctuations\n",
        "  punc = '''!()-[]{};:'\"\\,<>./?@#$%^&+*_~0123456789'''\n",
        "  for element in sentence:  \n",
        "    if element in punc:\n",
        "      sentence = sentence.replace(element, ' ')\n",
        "  # split the sentence into an array of words\n",
        "  words = sentence.split()\n",
        "  sentence = []\n",
        "  # remove the stop words\n",
        "  for word in words:\n",
        "    # word = spell(word) # this would have corrected spelling in case of incorrect spelling\n",
        "    if word not in stop_words:\n",
        "      sentence.append(word)\n",
        "  return sentence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1imZAd5j_Ei"
      },
      "source": [
        "Now, let us utilize the functions that we have made above to initialize the data obtained from the txt file into a Pandas dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "J1KYWIim2A11",
        "outputId": "9dbc09c8-a4da-4f20-8e58-249e74909a0a"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sentence</th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[no, way, plug, us, unless, go, converter]</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[good, case, excellent, value]</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[great, jawbone]</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[tied, charger, conversations, lasting, minute...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[mic, great]</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            Sentence Label\n",
              "0         [no, way, plug, us, unless, go, converter]     0\n",
              "1                     [good, case, excellent, value]     1\n",
              "2                                   [great, jawbone]     1\n",
              "3  [tied, charger, conversations, lasting, minute...     0\n",
              "4                                       [mic, great]     1"
            ]
          },
          "execution_count": 40,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Initialize empty array that will store our data\n",
        "data = []\n",
        "\n",
        "for i in range(count):\n",
        "  # Extract the sentence and label from raw data\n",
        "  sentence, label = remove_label(raw_data[i])\n",
        "  # Remove stop words, numbers and punctuations and convert to array\n",
        "  sentence = preprocess_text(sentence)\n",
        "  data.append([sentence, label])\n",
        "\n",
        "# Convert the data obtained to a Pandas dataframe\n",
        "df = pd.DataFrame(data, columns = ['Sentence', 'Label'])\n",
        "\n",
        "# Show the first few entries in our dataframe\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5pBCk0IjJRB"
      },
      "source": [
        "### Functions for Count Vectorization and calculating Accuracy\n",
        "The following functions have been declared in the next code cell that help in the formation fo a count vector for a sentence. The description for each function has been given alongside the code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "973lTcGBZhlc"
      },
      "outputs": [],
      "source": [
        "def get_unique_words(sentences):\n",
        "  \"\"\"\n",
        "  Input: An array of sentences(array of words) obtained after preprocessing\n",
        "  Output: A dictionary of unique words, each with an assigned index to define the order\n",
        "  \"\"\"\n",
        "  # A dictionary to track whether a word has been seen before or not\n",
        "  dic = {}\n",
        "  # Initialize an empty set of unique words\n",
        "  unique_words = {}\n",
        "  index = 0\n",
        "  for sentence in sentences:\n",
        "    for word in sentence:\n",
        "      if word not in dic:\n",
        "        dic[word] = 1\n",
        "        unique_words[word] = index\n",
        "        index += 1\n",
        "  return unique_words\n",
        "\n",
        "def get_matrix_for_sentences(unique_words, sentences):\n",
        "  \"\"\"\n",
        "  Input: The dictionary of unique words obtained from get_unique_words() (above),\n",
        "          an array of preprocessed sentences (each sentence is an array of words)\n",
        "  Output: A D x N matrix consisting of vector representation where each column is the vector representation of each sentence\n",
        "          (D: Number of unique words/Dimension of vector for each sentence, N: Number of sentences in input)\n",
        "  \"\"\"\n",
        "  matrix_data = np.array([])\n",
        "  for sentence in sentences:\n",
        "    vectorized_sentence = np.zeros(len(unique_words), dtype = int)# creates an array filled with zero of the length unique words\n",
        "    for word in sentence:\n",
        "      if word in unique_words.keys():\n",
        "        vectorized_sentence[unique_words[word]] += 1 #array[index]++\n",
        "    matrix_data = np.append(matrix_data, vectorized_sentence)#appends n times\n",
        "  return matrix_data.reshape([-1, len(unique_words)]).T #corrects the shape\n",
        "\n",
        "def create_training_testing_data(testing_fold):\n",
        "  \"\"\"\n",
        "  Input: A 0-indexed integer representing the index of fold to make the testing data from (range 0...6)\n",
        "  Output: A tuple consisting of (testing_df, training_df, unique_words),\n",
        "          where testing_df and training_df represent the testing and training data respectively after partitioning df\n",
        "          and unique_words is a dictionary consisting of the unique words for the corresponding training data.\n",
        "  \"\"\"\n",
        "  # Get testing_fold th fold from the df\n",
        "  testing_df = df.iloc[count*(testing_fold)//FOLD_COUNT:count*(testing_fold+1)//FOLD_COUNT, :] #indexes of the testing data\n",
        "  # Training data will be the rest of the data in df, but not in testing_df\n",
        "  training_df = df[~df.index.isin(testing_df.index)] \n",
        "\n",
        "  # Get the sentences in the training_df\n",
        "  sentences = list(training_df.Sentence)\n",
        "  # Get the unique words in the training_df\n",
        "  unique_words = get_unique_words(sentences)\n",
        "  \n",
        "  return (testing_df, training_df, unique_words)\n",
        "\n",
        "def evaluate_accuracy(labels, predictions):\n",
        "  \"\"\"\n",
        "  Input: Arrays of labels and predictions, each containing either '0' or '1'\n",
        "  Output: The fraction of predictions that correctly match the label\n",
        "  \"\"\"\n",
        "  # Make sure that the lengths of both arrays is equal\n",
        "  assert(len(labels) == len(predictions))\n",
        "  # Score will count how many predictions are correct\n",
        "  score = 0\n",
        "  for i in range(len(labels)):\n",
        "    label = labels[i]\n",
        "    prediction = predictions[i]\n",
        "    if label == prediction:\n",
        "      # If prediction is correct, increment score\n",
        "      score += 1\n",
        "  # Return accuracy\n",
        "  return (float(score) / float(len(labels)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQIXcpJtlqYn"
      },
      "source": [
        "### Function for fitting and evaluating the model\n",
        "The following function loops through the 7 different folds, creates and fits the training data and evaluates the accuracy for each fold. Finally, it takes the mean of all the accuracies and reports it as the final accuracy of the model.\n",
        "\n",
        "One major assumption of the model (and using count vectorization) is that words occur in product reviews conditionally independent of each other.\n",
        "\n",
        "We use the following formula obtained from Bayes Theorem to predict whether the sentence is of positive sentiment:\n",
        "<br><br>\n",
        "$$\n",
        "P(+\\mid Sentence) = \\frac{P(Sentence \\mid +)\\ P(+)}{P(Sentence)}\n",
        "$$\n",
        "where:\n",
        "$$\n",
        "P(Sentence) = P(Sentence \\mid +)P(+) + P(Sentence \\mid -)P(-)\n",
        "$$\n",
        "<br>\n",
        "\n",
        "Now under our assumption that the occurence of words in a sentence is conditionally independent, we can say that:\n",
        "\n",
        "$$\n",
        "P(Sentence \\mid +) = \\prod_{word\\ \\in\\ Sentence}^{} P(word \\mid +)\n",
        "$$\n",
        "and\n",
        "$$\n",
        "P(Sentence \\mid -) = \\prod_{word\\ \\in\\ Sentence}^{} P(word \\mid -)\n",
        "$$\n",
        "<br><br>\n",
        "To find the probabilty that a word appears given the class, we can use the **Laplace Smoothing formula**:\n",
        "\n",
        "$$\n",
        "P(word \\mid +) = \\frac{count(word, +) + k}{count(+) + k|V|}\n",
        "$$\n",
        "where:<br>\n",
        "$count(word, +)$ is the number of times that word occurs in the positive training sentences,<br>\n",
        "$k$ is a Laplace Smoothing constant,<br>\n",
        "$count(+)$ is the total number of words occuring in the positive class,<br>\n",
        "$|V|$ is the number of words occuring in our vocabulary.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ISHy82XvSIZJ"
      },
      "outputs": [],
      "source": [
        "def fit_and_evaluate(laplace_k):\n",
        "\n",
        "  # Sum of accuracies to divide later to find mean\n",
        "  accuracies_sum = 0.\n",
        "  for i in range(FOLD_COUNT):\n",
        "\n",
        "    print(f\"Building and evaluating model with testing fold: {i + 1}/{FOLD_COUNT}\")\n",
        "\n",
        "    # Obtain the testing, training data and unique words for the given fold index\n",
        "    testing_df, training_df, unique_words = create_training_testing_data(i)\n",
        "\n",
        "    # Number of unique words will be the number of words in our vocabulary\n",
        "    n_vocabulary = len(unique_words)\n",
        "\n",
        "    # Separate the positive and negative vectors into two separate dataframes\n",
        "    training_positive_df = training_df.loc[training_df['Label'] == '1']\n",
        "    training_negative_df = training_df.loc[training_df['Label'] == '0']\n",
        "\n",
        "    # Get only the sentences from the positive and negative dataframes\n",
        "    sentences_positive = list(training_positive_df.Sentence)\n",
        "    sentences_negative = list(training_negative_df.Sentence)\n",
        "\n",
        "\n",
        "    # Get a count vectorization matrix for positive and negative sentences over the unique_words\n",
        "    vectors_positive = get_matrix_for_sentences(unique_words, sentences_positive)\n",
        "    vectors_negative = get_matrix_for_sentences(unique_words, sentences_negative)\n",
        "\n",
        "    # Get the total number of positive and negative sentences in training\n",
        "    n_positive = np.sum(vectors_positive)\n",
        "    n_negative = np.sum(vectors_negative)\n",
        "\n",
        "    # Get the prior probabilities for use in the Bayes theorem formula\n",
        "    p_pos = n_positive / (n_positive + n_negative)\n",
        "    p_neg = n_negative / (n_positive + n_negative)\n",
        "\n",
        "    # Get the testing sentences and labels from the testing dataframe\n",
        "    testing_sentences = list(testing_df.Sentence) \n",
        "    testing_labels = list(testing_df.Label)\n",
        "\n",
        "    # Get the matrix represenation for testing sentences\n",
        "    testing_sent_vectors = get_matrix_for_sentences(unique_words, testing_sentences)\n",
        "\n",
        "    # Get a list of probabilities that a word in the training set appears in positive or negative sentences\n",
        "    # This basically goes over every single word from unique_words and calculates the probability\n",
        "    # that that word appears in positive and negative training data\n",
        "    # We utilize the Laplace smoothing formula\n",
        "    prob_pos_words = ((np.sum(vectors_positive, axis = 1) + laplace_k) / (n_positive + laplace_k * n_vocabulary)).reshape([-1, 1])\n",
        "    prob_neg_words = ((np.sum(vectors_negative, axis = 1) + laplace_k) / (n_negative + laplace_k * n_vocabulary)).reshape([-1, 1])\n",
        "    \n",
        "    # Now that we have obtained probabilies for each word given the class, we can multiply that\n",
        "    # with the testing vector to obtain probabilities that each word in testing set occurs in the corresponding class\n",
        "    # Next we will need to multiply all the words in the testing sentences together (because of our assumption)\n",
        "    # To do so, we must replace all the 0s in the testing sentences to 1 to prevent the absence of certain words\n",
        "    # from turning our probility that a testing sentence occurs in a particular class to 0.\n",
        "    prob_words_pos_testing = prob_pos_words * testing_sent_vectors\n",
        "    prob_words_pos_testing[prob_words_pos_testing == 0] = 1\n",
        "    prob_sentence_pos = np.prod(prob_words_pos_testing, axis = 0)\n",
        "    prob_words_neg_testing = prob_neg_words * testing_sent_vectors\n",
        "    prob_words_neg_testing[prob_words_neg_testing == 0] = 1\n",
        "    prob_sentence_neg = np.prod(prob_words_neg_testing, axis = 0)\n",
        "\n",
        "    # Now we apply the Bayes Theorem formula to obtain the probability that the sentences in testing class\n",
        "    # are of positive sentiment\n",
        "    prob_pos_sentence = (prob_sentence_pos * p_pos) / ((prob_sentence_pos * p_pos) + (prob_sentence_neg * p_neg))\n",
        "\n",
        "    # Get a result array to store either '1' or '0' depending on the probability obtained\n",
        "    result = []\n",
        "    for val in prob_pos_sentence:\n",
        "      if val >= 0.5:\n",
        "        result.append('1')\n",
        "      else:\n",
        "        result.append('0')\n",
        "      \n",
        "    # Now, let us evaluate the accuracy obtained for this fold\n",
        "    accuracy = evaluate_accuracy(testing_labels, result)\n",
        "\n",
        "    print(f'Completed for testing fold {i + 1} with accuracy: {accuracy}')\n",
        "\n",
        "    # Add the accuracy to the total sum\n",
        "    accuracies_sum += accuracy\n",
        "  \n",
        "  # Report the final mean accuracy of the model\n",
        "  print(f'\\n\\nMean Accuracy = {accuracies_sum / FOLD_COUNT}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYNjsmRdv4tm"
      },
      "source": [
        "Now that we have written the ```fit_and_evaluate``` function, let us call it and evaluate our model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JqFw-q99v0pg",
        "outputId": "dbeb6dcc-b90c-4948-ef41-4767b4df888f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Building and evaluating model with testing fold: 1/7\n",
            "Completed for testing fold 1 with accuracy: 0.8380281690140845\n",
            "Building and evaluating model with testing fold: 2/7\n",
            "Completed for testing fold 2 with accuracy: 0.8951048951048951\n",
            "Building and evaluating model with testing fold: 3/7\n",
            "Completed for testing fold 3 with accuracy: 0.7972027972027972\n",
            "Building and evaluating model with testing fold: 4/7\n",
            "Completed for testing fold 4 with accuracy: 0.8181818181818182\n",
            "Building and evaluating model with testing fold: 5/7\n",
            "Completed for testing fold 5 with accuracy: 0.7832167832167832\n",
            "Building and evaluating model with testing fold: 6/7\n",
            "Completed for testing fold 6 with accuracy: 0.8111888111888111\n",
            "Building and evaluating model with testing fold: 7/7\n",
            "Completed for testing fold 7 with accuracy: 0.7552447552447552\n",
            "\n",
            "\n",
            "Mean Accuracy = 0.8140240041648491\n"
          ]
        }
      ],
      "source": [
        "fit_and_evaluate(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-yTRIVlwQ6t"
      },
      "source": [
        "### Report\n",
        "Accuracy for fold 1: **83.80%**<br>\n",
        "Accuracy for fold 2: **89.51%**<br>\n",
        "Accuracy for fold 3: **79.72%**<br>\n",
        "Accuracy for fold 4: **81.81%**<br>\n",
        "Accuracy for fold 5: **78.32%**<br>\n",
        "Accuracy for fold 6: **81.11%**<br>\n",
        "Accuracy for fold 7: **75.52%**<br>\n",
        "<br>\n",
        "### Mean Accuracy: **81.40%**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_NdmZtTnytR9"
      },
      "source": [
        "## Limitations of Naive Bayes Classifier:\n",
        "- We have made the assumption that the occurence of words is conditionally independent of each other. However, in real life, this isn't always true as the occurence of some words may depend on the occurence of other words.\n",
        "- If the model encounters a word in the testing set that wasn't a part of the training data, it will not be able to make a proper prediction. This is known as the **Zero Frequency Problem**.\n",
        "- Naive Bayes requires a relatively larger training dataset to make accurate predictions.\n",
        "- Using generative models like Naive Bayes is usually more complex than using a discriminative model as we have to learn how the data is formed and what makes the data part of a particular class instead of just learning what differentiates the classes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87k_5G9CxhTV"
      },
      "source": [
        "## Extra: Building a model with all data as training data and user-provided reviews\n",
        "The code given below uses the same method provided above to fit the Naive Bayes classifier model with all the data provided as training data. It can be used to evaluate user-provided reviews and tries to classify them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EP4oBGzhPy7u"
      },
      "outputs": [],
      "source": [
        "# Building a model with all training data\n",
        "def get_optimized_params():\n",
        "    unique_words = get_unique_words(list(df.Sentence))\n",
        "    n_vocabulary = len(unique_words)\n",
        "    training_positive_df = df.loc[df['Label'] == '1']\n",
        "    training_negative_df = df.loc[df['Label'] == '0']\n",
        "    sentences_positive = list(training_positive_df.Sentence)\n",
        "    sentences_negative = list(training_negative_df.Sentence)\n",
        "    vectors_positive = get_matrix_for_sentences(unique_words, sentences_positive)\n",
        "    vectors_negative = get_matrix_for_sentences(unique_words, sentences_negative)\n",
        "    n_positive = np.sum(vectors_positive)\n",
        "    n_negative = np.sum(vectors_negative)\n",
        "    p_pos = n_positive / (n_positive + n_negative)\n",
        "    p_neg = n_negative / (n_positive + n_negative)\n",
        "    return (vectors_positive, vectors_negative, n_positive, n_negative, n_vocabulary, unique_words, p_pos, p_neg)\n",
        "\n",
        "vectors_positive, vectors_negative, n_positive, n_negative, n_vocabulary, unique_words, p_pos, p_neg = get_optimized_params()\n",
        "\n",
        "def evaluate(sentences):\n",
        "\n",
        "  testing_sentences = []\n",
        "  for sentence in sentences:\n",
        "    testing_sentences.append(preprocess_text(sentence))\n",
        "\n",
        "  testing_sent_vectors = get_matrix_for_sentences(unique_words, testing_sentences)\n",
        "  prob_pos_words = ((np.sum(vectors_positive, axis = 1) + 1) / (n_positive + 1 * n_vocabulary)).reshape([-1, 1])\n",
        "  prob_neg_words = ((np.sum(vectors_negative, axis = 1) + 1) / (n_negative + 1 * n_vocabulary)).reshape([-1, 1])\n",
        "  prob_words_pos_testing = prob_pos_words * testing_sent_vectors\n",
        "  prob_words_pos_testing[prob_words_pos_testing == 0] = 1\n",
        "  prob_sentence_pos = np.prod(prob_words_pos_testing, axis = 0)\n",
        "  prob_words_neg_testing = prob_neg_words * testing_sent_vectors\n",
        "  prob_words_neg_testing[prob_words_neg_testing == 0] = 1\n",
        "  prob_sentence_neg = np.prod(prob_words_neg_testing, axis = 0)\n",
        "\n",
        "  prob_pos_sentence = (prob_sentence_pos * p_pos) / ((prob_sentence_pos * p_pos) + (prob_sentence_neg * p_neg))\n",
        "  result = []\n",
        "  for val in prob_pos_sentence:\n",
        "    if val >= 0.5:\n",
        "      result.append('1')\n",
        "    else:\n",
        "      result.append('0')\n",
        "  return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_judCIv7x-p5"
      },
      "source": [
        "Now that we have the ```evaluate``` function, let us test it with some reviews."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DjzOKk8LuN_j",
        "outputId": "1b56ca9d-4d41-4934-a431-d85bc153ea13"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['1', '0', '0', '1']\n"
          ]
        }
      ],
      "source": [
        "print(evaluate([\"Easy to use\", \"i dont like it\", \"Money wasted\", \"I like it\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8s81EtIyG8E"
      },
      "source": [
        "As we can see, the model has correctly identified the class of the reviews."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Copy of Bayes.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
